# -*- coding: utf-8 -*-
"""simple_scipt_running_300925.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/171B9EiBK-YOHcOL_6cZ4DD3YjMZTVQyG
"""

# -*- coding: utf-8 -*-
import os
import shutil
import pprint
import pandas as pd
import os
import torch
import numpy as np
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
from torch.utils.data import Dataset
import torch.nn as nn
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
import torch.optim
import torchmetrics
#cross-validation
from sklearn.model_selection import KFold
from torch.utils.data import Subset
from sklearn.model_selection import TimeSeriesSplit
import random
from torchinfo import summary
import argparse

class EnergyDataset():

  def __init__(self, csv_path_train, csv_path_test):
    #load data
    self.data_train=pd.read_csv(csv_path_train, index_col='DateCET/CEST')
    self.data_test=pd.read_csv(csv_path_test, index_col='DateCET/CEST')


    self.features = ['price_energy[EUR/MWh]', 'is_peak', 'load_energy[MW]', 'wind_energy_generation[MWh]', 'is_holiday', 'price_carbon_permits[EUR]',
       'price_gas[EUR/m3]', 'pv_energy_generation[MWh]', 'price_coal[EUR]']
    self.target = 'price_energy[EUR/MWh]'

    X_train = self.data_train.loc[:, self.features].astype('float32')
    y_train = self.data_train.loc[:, self.target].astype('float32')

    X_test = self.data_test.loc[:, self.features].astype('float32')
    y_test = self.data_test.loc[:, self.target].astype('float32')

     # scalers
    self.scaler_X = MinMaxScaler()
    self.scaler_y = MinMaxScaler()

    # fit scalers on train data and transofrm both, train and test
    X_train_scaled, y_train_scaled = self.fit_scalers(X_train, y_train)
    X_test_scaled, y_test_scaled = self.transform_test(X_test, y_test)

    X_train_seq, y_train_seq = self.create_sequence_df(X_train_scaled, y_train_scaled)
    X_test_seq, y_test_seq = self.create_sequence_df(X_test_scaled, y_test_scaled)

    #tensorDataset
    self.train_dataset=TensorDataset(
        torch.tensor(X_train_seq, dtype=torch.float32),
        torch.tensor(y_train_seq, dtype=torch.float32))

    self.test_dataset=TensorDataset(
        torch.tensor(X_test_seq, dtype=torch.float32),
        torch.tensor(y_test_seq, dtype=torch.float32))

  def fit_scalers(self, X_train, y_train):
    X_scaled = self.scaler_X.fit_transform(X_train)
    y_scaled = self.scaler_y.fit_transform(y_train.values.reshape(-1,1))
    return X_scaled, y_scaled

  def transform_test(self, X_test, y_test):
    X_scaled = self.scaler_X.transform(X_test)
    y_scaled = self.scaler_y.transform(y_test.values.reshape(-1,1))
    return X_scaled, y_scaled

  def create_sequence_df(self, X, y, seq_length=24):
    xs, ys = [], []
    for i in range(len(X)-seq_length):
      xs.append(X[i:i+seq_length])
      ys.append(y[i+seq_length])

    return np.array(xs), np.array(ys)

  def inverse_targets(self, y_scaled):
    return self.scaler_y.inverse_transform(y_scaled)

#LSTM
class Net(nn.Module):
  def __init__(self, input_size, hidden_size, device):
    super().__init__()
    self.hidden_size=hidden_size
    self.device=device
    self.lstm = nn.LSTM(
        input_size=input_size,
        hidden_size=hidden_size,
        num_layers=1,
        batch_first=True,

    )

    self.fc=nn.Linear(hidden_size,1)
    self.dropout=nn.Dropout(0.2)
    self.hidden = None

  def reset_hidden(self, batch_size):
    self.hidden = (
      torch.zeros(1, batch_size, self.hidden_size).to(self.device),
      torch.zeros(1, batch_size, self.hidden_size).to(self.device)
    )

  def forward(self, x):
    if self.hidden is None:
      self.reset_hidden(x.size(0))

    out, self.hidden = self.lstm(x, self.hidden)
    self.hidden = (self.hidden[0].detach(), self.hidden[1].detach())

    out = out[:, -1, :]
    out = out / (torch.norm(out, dim=1, keepdim=True) + 1e-6)
    #print(f"Last timestep output shape: {out.shape}, min: {out.min().item()}, max: {out.max().item()}")
    out = self.dropout(out)
    out = self.fc(out)
    #print(f"FC output shape: {out.shape}, min: {out.min().item()}, max: {out.max().item()}")

    return out

class EarlyStopping:
    def __init__(self, patience, delta, verbose=False):
        self.patience = patience
        self.delta = delta
        self.verbose = verbose
        self.best_loss = None
        self.no_improvement_count = 0
        self.stop_training = False

    def check_early_stop(self, val_loss):
        if self.best_loss is None or val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.no_improvement_count = 0
            print(f'Validation improved')

        else:
            self.no_improvement_count += 1
            print(f"No improvement ({self.no_improvement_count}/{self.patience})")

            if self.no_improvement_count >= self.patience:
                self.stop_training = True
                if self.verbose:
                    print("Stopping early as no improvement has been observed.")

#class with training+validation+test
class Trainer:
  def __init__(self, dataset_train, batch_size, epochs, learning_rate, hidden_size, device):

    self.dataset_train = dataset_train
    self.batch_size = batch_size
    self.epochs = epochs
    self.learning_rate = learning_rate
    self.hidden_size = hidden_size

    self.dataloader_train = DataLoader(self.dataset_train, self.batch_size, shuffle=False, drop_last=True)

    self.device=device
    self.input_size = self.dataset_train[0][0].shape[1]

    self.model = Net(self.input_size, self.hidden_size, self.device)
    self.criterion = nn.MSELoss()
    self.optimizer = torch.optim.Adam(self.model.parameters(), self.learning_rate)

  def train(self):
    self.model.train()

    running_loss = 0.0
    for batch_idx, batch in enumerate(self.dataloader_train):

      inputs, labels = batch[0].to(self.device), batch[1].to(self.device)
      self.model.reset_hidden(inputs.size(0))

      #prediction
      outputs = self.model(inputs)
      outputs = outputs.view(-1,1)

      self.optimizer.zero_grad()

      loss = self.criterion(outputs, labels)

      #backpropagation
      loss.backward()
      self.optimizer.step()

      running_loss += loss.item()

    #running_loss/len(dataloader_train)
    return running_loss/len(self.dataloader_train)


  # def validate(self, dataloader_val):
  #   self.model.eval()
  #   self.model.reset_hidden(self.batch_size)

  #   val_loss = 0.0
  #   with torch.no_grad():
  #     for batch in dataloader_val:
  #       inputs, labels = batch[0].to(self.device), batch[1].to(self.device)
  #       outputs= self.model(inputs)
  #       outputs=outputs.reshape(-1,1)
  #       val_loss += self.criterion(outputs, labels).item()

  #   avg_val_loss = val_loss/len(dataloader_val)
  #   return val_loss/len(dataloader_val)

  def plot_losses(self, train_losses):
    plt.figure(figsize=(8,5))
    plt.plot(train_losses, label='train')
    #plt.plot(val_losses, label='val')
    plt.title('model_loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend()
    plt.savefig(f"loss_curve.png")

  def plot_residuals(y_true, y_pred):
    residuals = y_pred - y_true

      # Scatter plot: residuals vs true values
    plt.figure(figsize=(8,5))
    plt.scatter(y_true, residuals, alpha=0.6)
    plt.axhline(0, color='red', linestyle='--', linewidth=1)
    plt.xlabel("True values")
    plt.ylabel("Residuals (Predicted - True)")
    plt.title("Residuals vs True values")
    plt.show()

    # Histogram of residuals
    plt.figure(figsize=(8,5))
    plt.hist(residuals, bins=30, edgecolor='k', alpha=0.7)
    plt.axvline(0, color='red', linestyle='--', linewidth=1)
    plt.xlabel("Residual")
    plt.ylabel("Frequency")
    plt.title("Distribution of Residuals")
    plt.show()

  def fit(self):

    train_losses = []
    val_losses = []

    for epoch in range(self.epochs):
      #train
      avg_running_loss=self.train()

      #validate
      #avg_val_loss=self.validate(dataloader_val)


      train_losses.append(avg_running_loss)
      #val_losses.append(avg_val_loss)

      #check early stopping condition
      # early_stopping.check_early_stop(avg_val_loss)
      # if early_stopping.stop_training:
      #   print(f"Early stopping  at epoch{epoch}.")
      #   break


      print(f"Epoch {epoch +1}/{self.epochs}, Train Loss: {avg_running_loss}")

    self.plot_losses(train_losses)

    return self.model

class Tester():
  def __init__(self, df_test, model, batch_size, device, energy_data):

    self.df_test = df_test
    self.model = model
    self.batch_size = batch_size
    self.device = device
    self.energy_data = energy_data
    #self.inverse_targets = inverse_targets

  def test_model(self):

    all_preds = []
    all_labels = []

    dataloader_test = DataLoader(self.df_test, self.batch_size, shuffle=False, num_workers=2, drop_last=True)


    self.model.eval()
    with torch.no_grad():
      for X, y in dataloader_test:
        X, y = X.to(self.device), y.to(self.device)
        preds = self.model(X)
        preds_original = self.energy_data.inverse_targets(preds.cpu().numpy())
        y_original = self.energy_data.inverse_targets(y.cpu().numpy())

        all_preds.append(preds_original)
        all_labels.append(y_original)

    all_preds = np.concatenate(all_preds, axis=0)
    all_labels = np.concatenate(all_labels, axis=0)

    return all_preds, all_labels


  def plot_prediction_labels(self, all_preds, all_labels):
    plt.figure(figsize=(12,6))

    plt.plot(all_preds, label='Prediction', color='orange')
    plt.plot(all_labels, label='Actual', color='blue')

    #plt.title(title, font_size)
    plt.legend()
    #plt.show()
    plt.savefig('actual_predict_plot.png', dpi=300)
    plt.close()

  def calculate_metrics(self, preds, labels):
    '''
    The function calculates metrics.
    '''
    mse = torchmetrics.MeanSquaredError()
    mae = torchmetrics.MeanAbsoluteError()
    rmse = torchmetrics.MeanSquaredError(squared=False)

    mse.update(preds, labels)
    mae.update(preds, labels)
    rmse.update(preds, labels)

    print(f"MSE: {mse.compute()}")
    print(f"MAE: {mae.compute()}")
    print(f"RMSE: {rmse.compute()}")
 


def main(args):

  energy_data = EnergyDataset(csv_path_train= args.train_path, csv_path_test=args.test_path)

  device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
  trainer = Trainer(energy_data.train_dataset, args.batch_size, args.epochs, args.learning_rate, args.hidden_size, device)

  print("Start training...")
  model = trainer.fit()

  print("Testing...")
  tester = Tester(energy_data.test_dataset, model, args.batch_size, device, energy_data)
  all_preds, all_labels = tester.test_model()
  print(all_preds)
  print(all_labels)

  tester.plot_prediction_labels(all_preds, all_labels)

  tester.calculate_metrics(torch.tensor(all_preds), torch.tensor(all_labels))

if __name__ == '__main__':

  parser = argparse.ArgumentParser(description='Energy Price Forecasting')

  parser.add_argument("-tr", "--train_path", type=str, help="Path to the train dataset")
  parser.add_argument("-te", "--test_path", type=str, help="Path to the test dataset")
  parser.add_argument("-hs", "--hidden_size", type=int, default=200, help="Number of hidden units in the LSTM layer")
  parser.add_argument("-e", "--epochs", type=int, default=30, help="Number of training epochs")
  parser.add_argument("-lr", "--learning_rate", type=float, default=0.01, help="Learning rate for the optimizer")
  parser.add_argument("-bs", "--batch_size", type=int, default=256, help="Batch size for training")
  parser.add_argument("-out_path", "--output_path", type=str, help="Path to save the trained model")

  args = parser.parse_args()

  main(args)